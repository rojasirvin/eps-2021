<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Errores estándar e inferencia</title>
    <meta charset="utf-8" />
    <meta name="author" content="Irvin Rojas" />
    <script src="libs/header-attrs-2.9/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="libs/cide.css" type="text/css" />
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css" type="text/css" />
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.7.2/css/all.css" type="text/css" />
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

class: title-slide



&lt;style type="text/css"&gt;
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 60% !important;
}
&lt;/style&gt;

.title[
# Clase 10. Errores estándar e inferencia
]
.subtitle[
## Evaluación de Programas
]
.author[
### Irvin Rojas &lt;br&gt; [rojasirvin.com](https://www.rojasirvin.com/) &lt;br&gt; [&lt;i class="fab fa-github"&gt;&lt;/i&gt;](https://github.com/rojasirvin) [&lt;i class="fab fa-twitter"&gt;&lt;/i&gt;](https://twitter.com/RojasIrvin) [&lt;i class="ai ai-google-scholar"&gt;&lt;/i&gt;](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&amp;hl=en)
]

.affiliation[
### Centro de Investigación y Docencia Económicas &lt;br&gt; División de Economía
]

---
# Agenda

1. En esta sesión nos concentraremos en la estimación de errores estándar

1. Definiremos los estimadores de la matriz de varianzas robusta empleadas por los programas más usados

1. Estudiaremos el uso de rutinas bootstrap para la estimación de errores estándar

1. Estudiaremos las implicaciones de los datos agrupados en la estimación de errores estándar

---

class: inverse, middle, center

# Errores estándar no estándar

---

# Errores estándar robustos

- Recordemos que con errores homocedásticos, la matriz de varianzas del estimador de MCO puede ser estimada como:

`$$\hat{V}(\beta_{MCO}^H)=\hat{\sigma}^2(X'X)^{-1}$$`
donde `\(\hat{\sigma}^2=\frac{1}{N-k}\hat{u}_i^2\)` y `\(\hat{u}_i^2=(y_i-X_i'\hat{\beta}_{MCO})^2\)`

--

- Una primera *desviación*  respecto a los errores clásicos ocurre cuando relajamos el supuesto de homocedasticidad

- En la [clase 3](https://eps-2021.netlify.app/clases/clase_5.html#20) estudiamos de manera general las propiedades asintóticas del estimador de MCO

- La varianza asintótica es:

`$$V(\hat{\beta}_{MCO}^{R})=(X'X)^{-1}X'\Omega X(X'X)^{-1}$$`



---

# Errores robustos a la heterocedasticidad

- Un estimador de la varianza del estimador de MCO que no asume homocedasticidad es el estimador propuesto por White (1980)

- En la [clase 3](https://eps-2021.netlify.app/clases/clase_5.html#21) le dimos la forma de:

`$$\hat{V}(\beta_{MCO}^R)=(X'X)^{-1}\left(\sum_i\hat{u}_i^2x_ix_i'\right)(X'X)^{-1}$$`

- [Aquí un recordatorio](http://mlwiki.org/index.php/Matrix-Matrix_Multiplication) de por qué podemos escribir `\(X'uu'X\)` como una sumatoria

- Consideremos la *carnita* del sándiwch

`$$\sum_i\hat{u}_i^2x_ix_i \equiv \sum_i \hat{\psi}_i x_ix_i'$$`


---

# Errores estándar robustos

- Dependiendo de cómo se especifique `\(\hat{\psi}_i\)`, obtenemos distintas versiones del estimador de varianzas robusto

- La propuesta de White original es:

`$$HC0:\quad\hat{\psi}_i=\hat{u}_i^2$$`
- Este estimador asintóticamente consistente

--

- En muestras pequeñas, muchas veces se emplea la siguiente corrección:

`$$HC1:\quad\hat{\psi}_i=\frac{N}{N-k}\hat{u}_i^2$$`
---

# Desviación a la influencia

- Un par de resultados nos ayudarán a entender qué hacen las otras correcciones a la matriz robusta en el software

- Definimos la **influencia** de la observación `\(i\)` como:

`$$h_{ii}=X_i'(X'X)^{-1}X_i$$`

- `\(h_{ii}\)` nos dice qué tanto *jala* la observación `\(i\)` a la línea de regresión

- En una regresión con un solo regresor `\(x\)`, se puede mostrar que la influencia de la observación `\(i\)` es:

`$$h_{ii}=\frac{1}{N}+\frac{(x_i-\bar{x})^2}{\sum(x_j-\bar{x})^2}$$`
es decir, que la influencia se incrementa cuando `\(x_i\)` se aleja de la media

- La influencia es un número entre 0 y 1 y además `\(\sum_i h_{ii}=k\)`, siendo `\(k\)` el número de regresores

---

# Errores estándar robustos

- Algunos autores sugieren usar la influencia en la matriz de varianzas robusta

- Se proponen algunas alternativas:

`$$HC2:\quad\hat{\psi}_i=\frac{1}{1-h_{ii}}\hat{u}_i^2$$`

`$$HC3:\quad\hat{\psi}_i=\frac{1}{(1-h_{ii})^2}\hat{u}_i^2$$`

--

- Long &amp; Ervin (2000) realizaron un experimento de simulación y recomendaron usar `\(HC3\)` en muestras pequeñas, por lo que el paquete *sandwich* en R usa `\(HC3\)` por default

- Es importante tener en cuenta qué tipo de errores estándar piden que el software calcule

---

class: inverse, middle, center

# Errores agrupados

---

# Errores agrupados

- Surgen naturalmente cuando las observaciones están agrupadas

  - Niños en salones de clase
  - Hogares en localidades
  - Solicitudes de empleo en una empresa
  - Ahorradoras en un banco

- El supuesto de errores independientes claramente no se cumple

--

- Pensemos en un problema simple para entender la intución:

`$$y_{ig}=\beta_0+\beta_1 x_g+e_{ig}$$`

- Aquí, `\(x_g\)` es un regresor que es el mismo para todos los miembros del grupo `\(g\)`

- Asumamos que todos los grupos tienen tamaño `\(n\)`

---

# Errores agrupados

- Podemos mostrar que la correlación de errores entre dos observaciones `\(i\)` y `\(j\)` que pertenecen a `\(g\)` es `$$E(e_{ig}e_{jg})=\overbrace{\rho_e}^{\substack{\text{coeficiente de correlación} \\ \text{intraclase residual}}} \underbrace{\sigma_e^2}_{\text{varianza residual}}$$`

- Le damos una estructura aditiva a los errores:

`$$e_{ig}=\nu_g+\eta_{ig}$$`
donde `\(\nu_g\)` captura toda la correlación dentro del grupo

- `\(\eta_{ig}\)` es un error idiosincrático con media cero e independiente de cualquier otro `\(\eta_{jg}\)`

- Como queremos analizar el problema del agrupamiento, asumimos que tanto `\(v_g\)` y `\(\eta_{ig}\)` son homocedásticos


---

# Errores agrupados

- Con esta estructura de errores, el coeficiente de correlación intraclase es:

`$$\rho_e=\frac{\sigma_{\nu}^2}{\sigma_{\nu}^2+\sigma_{\eta}^2}$$`
- Deberíamos calcular la matriz de varianzas `\(V_C(\hat{\beta})\)` tomando en cuenta esta estructura


--

- ¿Qué pasa si hacemos MCO en el contexto de este problema?


- Moulton (1984) muestra que:

`$$\frac{V_C(\hat{\beta})}{V_{MCO}(\hat{\beta})}=1+(n-1)\rho_e$$`
- A `\(\sqrt{\frac{V_C(\hat{\beta})}{V_{MCO}(\hat{\beta})}}\)` se le conoce como el *factor de Moulton*

---

# Factor de Moulton

- El factor de Moulton nos dice qué tanto sobreestimamos la precisión al ignorar la correlación intra clase

- Visto de otro modo:

`$$V_C(\hat{\beta})=\left(1+(n-1)\rho_e\right)V_{MCO}(\hat{\beta})$$`

- Es decir entre más grande sea la correlación dentro de los grupos, más deberíamos *inflar* los errores de MCO

--

- Consideremos el caso extremo de que `\(\rho_e=1\)`, es decir, que todas las `\(y_{ig}\)` dentro del mismo `\(g\)` son iguales

- Entonces el factor de Moulton es simplemente `\(\sqrt{n}\)`

- Visto de otro modo, la matriz de varianzas correcta se obtendría multiplicando por `\(n\)` la matriz `\(V_{MCO}(\hat{\beta})\)`

`$$V_C(\hat{\beta})=n V_{MCO}(\hat{\beta})$$`
---

# Errores agrupados en general

- En general, `\(x_{ig}\)` varía a nivel individual y tenemos grupos de tamaño `\(n_g\)`

- En este caso, el factor de Moulton es la raíz cuadrada de:

`$$\frac{V_C(\hat{\beta})}{V_{MCO}(\hat{\beta})}=1+\left(\frac{V(n_g)}{\bar{n}}+\bar{n}-1\right)\rho_x\rho_e$$`
donde `\(\bar{n}\)` es el tamaño promedio del grupo y `\(\rho_x\)` es la correlación intraclase de `\(x_{ig}\)`

- No es necesario asumir una forma para `\(\rho_x\)` (se puede calcular)

--

- Noten que el error que cometemos es más grande entre más heterogéneo es el tamaño de grupos y entre más grande es `\(\rho_x\)`

- Por tanto, cuando el tratamiento no varía entre grupos, este error es grande

---

# Soluciones para errores agrupados

- Solución paramétrica: calcular directamente el factor de Moulton e inflar los errores de MCO

- Bootstrap por bloques: en vez de hacer muestras bootrstrap remuestreando individuos, se remuestrean grupos

- Estimar los errores agrupados (*clustered standard errors*)

---

# Errores estándar agrupados

- Con errores agrupados podemos escribir el estimador de MCO como

$$
`\begin{aligned}
\hat{\beta}&amp;=\beta+(X'X)^{-1}X'u \\
&amp;=(X'X)^{-1}\left(\sum_{g=1}^G X_gu_g\right)
\end{aligned}`
$$
- Suponiendo independencia entre `\(g\)` y correlación dentro de cada grupo:

`$$E(u_{ig}u_{jg'}|x_{ig}x_{jg'})=0$$`
excepto cuando `\(g=g'\)`


- En este caso, el estimador de MCO tiene una varianza asintótica dada por

`$$V({\hat{\beta}}_{MCO})=(X'X)^{-1}\left(\sum_{g=1}^G X_g'u_gu_g'X\right)(X'X)^{-1}$$`
---

# Errores estándar agrupados

- Con errores heterocedásticos, pero sin agrupamiento, la matriz de varianzas de White (1980) tiene una estructura como sigue:

`$$\hat{V}(\hat{\beta}_{R})=(X'X)^{-1}X'\hat{\Sigma} X (X'X)^{-1}$$`

- Donde

`$$\hat{\Sigma}=\left(\begin{matrix} \hat{u}_{1}^2 &amp; 0  &amp; 0  &amp; \ldots &amp; 0 \\ 0 &amp; \hat{u}_{2}^2 &amp; 0 &amp; \ldots &amp; 0 \\ \vdots &amp; &amp; &amp; &amp; \\ 0 &amp; &amp; &amp;  \ldots &amp; \hat{u}_{n}^2\end{matrix}\right)$$`
---

# Errores estándar agrupados

- Para estimar la varianza con errores agrupados empleamos una generalización de la propuesta de White para errores robustos

- Si `\(G\to\infty\)`, el estimador de la matriz de errores agrupados robusta (CRVE) es consistente para estimar `\(V(\hat{\beta})\)`:

`$$\hat{V}_{CR}(\hat{\beta})=(X'X)^{-1}\left(\sum_{g=1}^G X_g'\hat{u}_g\hat{u}_g'X_g\right)(X'X)^{-1}$$`
donde `\(\hat{u}_g\hat{u}_g'\)` es la matriz de varianzas para los individuos del grupo `\(g\)`

- De manera compacta

`$$\hat{V}_{CR}(\hat{\beta})=(X'X)^{-1}X'\hat{\Sigma} X(X'X)^{-1}$$`

---

# Errores estándar agrupados

- Y en este caso la matriz `\(\hat{\Sigma}\)` tiene una estructura agrupada

`$$\small \hat{\Sigma}=\left(\begin{matrix} \hat{u}_{1,1}^2 &amp; \hat{u}_{1,1}\hat{u}_{2,1} &amp; \ldots &amp; \hat{u}_{1,1} \hat{u}_{n,1}&amp; 0 &amp; 0 &amp; \ldots &amp;  0 &amp; \ldots &amp; 0 &amp; 0 &amp; \ldots &amp;  0 \\ \hat{u}_{2,1}\hat{u}_{1,1} &amp; \hat{u}_{2,1}^2 &amp; \ldots &amp; \hat{u}_{2,1}\hat{u}_{n,1} &amp; 0 &amp; 0 &amp; \ldots &amp; 0 &amp; \ldots  &amp; 0 &amp; 0 &amp; \ldots &amp;  0\\ 
\vdots &amp; \vdots  &amp; &amp; \vdots &amp; \vdots &amp; \vdots  &amp; &amp;  \vdots&amp; &amp; \vdots &amp; \vdots &amp;  &amp;  \vdots \\ \hat{u}_{n,1}\hat{u}_{1,1} &amp; \hat{u}_{n,1}\hat{u}_{2,1}&amp; \ldots &amp; \hat{u}_{n,1}^2&amp; 0 &amp; 0 &amp;\ldots &amp; 0 &amp; \ldots &amp; 0 &amp; 0 &amp; \ldots &amp;  0 \\  0 &amp; 0 &amp; \ldots &amp;  0 &amp; \hat{u}_{1,2}^2 &amp; \hat{u}_{1,2}\hat{u}_{2,2} &amp; \ldots &amp; \hat{u}_{1,2}\hat{u}_{n,2} &amp;\ldots &amp; 0 &amp; 0 &amp; \ldots &amp;  0  \\ 0 &amp; 0 &amp; \ldots &amp;  0 &amp; \hat{u}_{2,2}\hat{u}_{1,2} &amp; \hat{u}_{2,2}^2 &amp; \ldots &amp; \hat{u}_{2,2}\hat{u}_{n,2} &amp;\ldots &amp; 0 &amp; 0 &amp; \ldots &amp;  0 \\ \vdots &amp; \vdots  &amp; &amp; \vdots &amp; \vdots &amp; \vdots  &amp; &amp;  \vdots&amp; &amp; \vdots &amp; \vdots &amp;  &amp;  \vdots  \\ 0 &amp; 0 &amp; \ldots &amp;  0 &amp; \hat{u}_{n,2}\hat{u}_{1,2} &amp; \hat{u}_{n,2}\hat{u}_{2,2} &amp; \ldots &amp; \hat{u}_{n,2}^2 &amp;\ldots &amp; 0 &amp; 0 &amp; \ldots &amp;  0 \\ \vdots &amp; \vdots  &amp; &amp; \vdots &amp; \vdots &amp; \vdots  &amp; &amp;  \vdots&am
